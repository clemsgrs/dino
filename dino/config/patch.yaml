train:
  dataset_path: Pathology:root=/root/data
  batch_size_per_gpu: 8
  pct:
  output_dir: 'output'
  experiment_name: 'patch'
  save_every: 5 # save checkpoint every x epochs
  seed: 0
  resume: false
  resume_from_checkpoint:
tune:
  tune_every: 1 # leave blank if you don't want to use early stopping mechanism
  early_stopping:
    enable: false
    tracking: 'auc'
    min_max: "max"
    patience: 20
    min_epoch: 30
  knn:
    nb_knn: 20
    temperature: 0.07
    query_dataset_path: KNN:root=/root/data:split=QUERY
    test_dataset_path: KNN:root=/root/data:split=TEST
    batch_size_per_gpu: 256
    num_workers: 8
    save_features: false
    use_cuda: true
student:
  arch: vit_small
  patch_size: 16
  out_dim: 65536 # dimensionality of the DINO head output. For complex and large datasets large values (like 65k) work well
  norm_last_layer: false # whether or not to weight normalize the last layer of the DINO head ; not normalizing leads to better performance but can make the training unstable.
  drop_path_rate: 0.1 # stochastic depth rate
  use_bn_in_head: false # whether to use batch normalizations in projection head
teacher:
  momentum_teacher: 0.996 # base EMA parameter for teacher update. The value is increased to 1 during training with cosine schedule ; we recommend setting a higher value with small batches: for example use 0.9995 with batch size of 256
  warmup_teacher_temp: 0.04 # initial value for the teacher temperature: 0.04 works well in most cases ; try decreasing it if the training loss does not decrease
  teacher_temp: 0.04 # final value (after linear warmup) of the teacher temperature ; for most experiments, anything above 0.07 is unstable ; we recommend starting with the default value of 0.04 and increase this slightly if needed
  warmup_teacher_temp_epochs: 0 # number of warmup epochs for the teacher temperature
optim:
  epochs: 100
  lr: 0.0005
  weight_decay: 1e-5
  warmup_epochs: 3  # number of epochs for the linear learning-rate warm up.
  clip_grad: 3.0  # maximal parameter gradient norm if using gradient clipping ; clipping with norm .3 ~ 1.0 can help optimization for larger ViT architectures. 0 for disabling
  freeze_last_layer_epochs: 1 # number of epochs during which we keep the output layer fixed ; typically doing so during the first epoch helps training ; try increasing this value if the loss does not decrease
  lr_scheduler:
    name: cosine
    min_lr: 1e-6 # target LR at the end of optimization ; we use a cosine LR schedule with linear warmup
    weight_decay: 0.04 # initial value of the weight decay ; with ViT, a smaller value at the beginning of training works well
    weight_decay_end: 0.4 # final value of the weight decay ; we use a cosine schedule for WD and using a larger decay by the end of training improves performance for ViTs
crops:
  global_crops_scale:
    - 0.4
    - 1.
  local_crops_number: 8
  local_crops_scale:
    - 0.05
    - 0.4
speed:
  use_fp16: true # whether or not to use half precision for training ; improves training time and memory requirements, but can provoke instability and slight decay of performance ; we recommend disabling mixed precision if the loss is unstable, if reducing the patch size or if training with bigger ViTs
  num_workers: 4
wandb:
  enable: false
  project: 'vision'
  username: 'vlfm'
  exp_name: '${experiment_name}'
  tags: ['dino', 'patch', '${model.arch}']
  dir: '/home/user'
  group:
  to_log: ['loss']
  resume_id:
hydra:
  run:
    dir: /tmp/hydra_output