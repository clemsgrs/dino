data_dir: "/maindisk/pathology-fm/discern/data/multi-center-prostate"
output_dir: "output/discern"

seed: 0
start_from_checkpoint:
resume: false
resume_from_dir:
resume_from_checkpoint:

model:
  arch: "vit_small"
  input_size: 256
  patch_size: 14
  out_dim: 65536 # dimensionality of the DINO head output. For complex and large datasets large values (like 65k) work well
  norm_last_layer: false # whether or not to weight normalize the last layer of the DINO head ; not normalizing leads to better performance but can make the training unstable.
  momentum_teacher: 0.996 # base EMA parameter for teacher update. The value is increased to 1 during training with cosine schedule ; we recommend setting a higher value with small batches: for example use 0.9995 with batch size of 256
  use_bn_in_head: false # whether to use batch normalizations in projection head
  warmup_teacher_temp: 0.04 # initial value for the teacher temperature: 0.04 works well in most cases ; try decreasing it if the training loss does not decrease
  teacher_temp: 0.04 # final value (after linear warmup) of the teacher temperature ; for most experiments, anything above 0.07 is unstable ; we recommend starting with the default value of 0.04 and increase this slightly if needed
  warmup_teacher_temp_pct: 0 # number of warmup epochs for the teacher temperature
  drop_path_rate: 0.1 # stochastic depth rate

# training/optimization parameters
training:
  nepochs: 50
  warmup_pct: 0.1 # percentage of iterations for the linear learning-rate warm up.
  freeze_last_layer_pct: 0.01 # percentage of iterations during which we keep the output layer fixed ; typically doing so during the first epoch helps training ; try increasing this value if the loss does not decrease
  batch_size_per_gpu: 32
  clip_grad: 3.0 # maximal parameter gradient norm if using gradient clipping ; clipping with norm .3 ~ 1.0 can help optimization for larger ViT architectures. 0 for disabling
  pct:
  save_every_pct: 0.1 # percentage of iterations at which we save the model

optim:
  name: "adamw" # type of optimizer ; we recommend using adamw with ViTs
  lr: 0.0005 # learning rate at the end of linear warmup (highest LR used during training) ; the learning rate is linearly scaled with the batch size, and specified here for a reference batch size of 256
  wd: 1e-5
  lr_scheduler:
    name: "cosine"
    min_lr: 1e-6 # target LR at the end of optimization ; we use a cosine LR schedule with linear warmup
    weight_decay: 0.04 # initial value of the weight decay ; with ViT, a smaller value at the beginning of training works well
    weight_decay_end: 0.4 # final value of the weight decay ; we use a cosine schedule for WD and using a larger decay by the end of training improves performance for ViTs

aug:
  global_crop_size: 224 # size of the global views
  local_crop_size: 96 # size of the local views
  # scale range of the cropped image before resizing, relatively to the origin image ; used for large global view cropping ; when disabling multi-crop (local_crops_number: 0), we recommand using a wider range of scale (global_crops_scale: (0.14, 1))
  global_crops_scale:
    - 0.4
    - 1.
  local_crops_number: 8 # number of small local views to generate. Set this parameter to 0 to disable multi-crop training ; when disabling multi-crop we recommend to use global_crops_scale = (0.14, 1)
  # scale range of the cropped image before resizing, relatively to the origin image ; used for small local view cropping of multi-crop
  local_crops_scale:
    - 0.05
    - 0.4
  solarization: false

speed:
  use_fp16: true # whether or not to use half precision for training ; improves training time and memory requirements, but can provoke instability and slight decay of performance ; we recommend disabling mixed precision if the loss is unstable, if reducing the patch size or if training with bigger ViTs
  num_workers: 4

early_stopping:
  enable: false
  tracking: "accuracy"
  min_max: "max"
  patience: 10
  min_epoch: 5
  save_every: 0  # set to N to save every N epochs, 0 to disable

tuning:
  enable: false
  tune_every: 5  # evaluate every N epochs
  image_path_col: "image_path"  # column name for image paths in benchmark CSV
  label_col: "label"  # column name for labels in benchmark CSV
  partition_col: "partition"  # column name for train/test partition

  benchmarks: []
  # Plugin-based orchestrator configuration (recommended).
  # Example:
  # plugins:
    # - type: "standard_probe"
    #   enable: false
    #   tune_every: 5
    #   image_path_col: "image_path"
    #   label_col: "label"
    #   partition_col: "partition"
    #   transforms:
    #     resize: 256
    #     crop_size: 224
    #   num_workers: 4
    #   batch_size_per_gpu: 64
    #   benchmarks:
    #     - name: "benchmark_a"
    #       csv_path: "/path/to/benchmark.csv"
    #       evaluator: "knn"
    #       primary: false
    #       knn: {k: 20, temperature: 0.07}
    #     - name: "benchmark_a"
    #       csv_path: "/path/to/benchmark.csv"
    #       evaluator: "linear"
    #       primary: false
    #       linear: {epochs: 100, lr: 0.01, batch_size: 256}
    # - type: "pathorob"
    #   enable: true
    #   tune_every: 1  # evaluate every 5 epochs to reduce overhead
    #   seed: 0
    #   max_pairs: 2  # limit pairs for fast debugging
    #   transforms:
    #     resize: 256
    #     crop_size: 224
    #   num_workers: 4
    #   batch_size_per_gpu: 64
    #   ri: # robustness index
    #     enable: true
    #     k_selection_policy: "paper_median_fixed"  # or "knn_bacc"
    #     default_k: 5  # small k for tiny dataset
    #     fixed_k:
    #       camelyon_tiny: 5  # tiny dataset k
    #     k_candidates: [3, 5, 7]  # small candidates for debugging
    #   apd: # average performance drop
    #     enable: true
    #     repetitions: 1  # minimal for tiny dataset debugging
    #     id_test_fraction: 0.1  # small fraction to keep more training samples
    #     correlation_levels: [0.0, 1.0]  # minimal for debugging
    #   clustering:
    #     enable: true
    #     repeats: 2  # minimal for debugging
    #     k_min: 2
    #     k_max: 5  # minimal for debugging
    #   datasets:
    #     camelyon_tiny:  # Using tiny dataset for debugging
    #       enable: true
    #       manifest_csv: "/data/pathology/projects/clement/code/dino/data/pathorob_camelyon_tiny/benchmark.csv"
    #       id_centers: ["RUMC", "UMCU"]
    #       ood_centers: ["CWZ"]
    #     tcga_2x2:
    #       enable: false
    #       manifest_csv: ""
    #       id_centers: []
    #       ood_centers: []
    #     tcga_4x4:
    #       enable: false
    #       manifest_csv: ""
    #       id_centers: []
    #       ood_centers: []
    #     tolkach_2x2:
    #       enable: false
    #       manifest_csv: ""
    #       id_centers: ["WNS", "CHA"]
    #       ood_centers: []
    #     tolkach_esca:
    #       enable: false
    #       manifest_csv: ""
    #       id_centers: ["WNS", "CHA", "UKK"]
    #       ood_centers: ["TCGA"]
  plugins: []

adversarial:
  enable: false
  max_lambda: 1.0
  gamma: 10.0
  warmup_pct: 0.1
  classifier:
    hidden_dims: [1024, 512]
    dropout: 0.5
  optim:
    lr: 0.001
    weight_decay: 0.0005
    clip_grad: 0  # 0 to disable, or max gradient norm for domain classifier

wandb:
  enable: false
  project: "discern"
  username: "clemsg"
  exp_name: "dino"
  tags: ["dino", "${model.arch}"]
  dir: "/home/user/"
  group:
  resume_id: